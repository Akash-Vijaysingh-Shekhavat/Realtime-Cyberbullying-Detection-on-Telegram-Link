# -*- coding: utf-8 -*-
"""Data Understanding & Cleaning- Cyberbullying Tweets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nZdP5mcchedd5yC2vkXpbS6MSOBNWsxd

# Preprocessing
"""

pip install stop_words

pip install emoji

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


import re, string
from stop_words import get_stop_words
import emoji


import nltk
from nltk import ngrams
from nltk.stem import WordNetLemmatizer,SnowballStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet

from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator

stemmer = SnowballStemmer("english")

from google.colab import drive
drive.mount('/content/drive')

"""#### Import dataset"""

df = pd.read_csv('cyberbullying_tweets.csv')

df

df = df.rename(columns={'tweet_text': 'Tweet', 'cyberbullying_type': 'cat_Type'})

df = df.drop(df[df.cat_Type == "other_cyberbullying"].index)

df.shape

df = df.drop_duplicates()

"""#### Charateristics of target variable"""

unique = df["cat_Type"].unique()
freq = df["cat_Type"].value_counts()
sns.set(font_scale=1)


df['cat_Type'] = pd.Categorical(df['cat_Type'], categories=unique)

ax = sns.countplot(x="cat_Type", data=df, order=df["cat_Type"].value_counts().index)

plt.title("Target variable counts in dataset")
plt.ylabel('Number of tweets')
plt.xlabel('Tweet Type')

rects = ax.patches
for rect, label in zip(rects, freq):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')

plt.show()

d = {"not_cyberbullying": 0, "religion": 1, "age": 2, "gender": 3, "ethnicity": 4}
df['Type'] = df['cat_Type'].map(d)

df

df.to_csv("preprocess_tweet.csv")

stop_words = get_stop_words('english')
stp_wrd = ['','r', 're', 'rt', 'didn', 'bc', 'n', 'm', 'im', 'll', 'y', 've', 'u', 'ur', 'don', 't', 's', 'b', 'ima',
                  'al', 'yeah', 'get', 'ppl', 'etc', 'http', 'tco']

stop_words.extend(stp_wrd)


def emoticon_remove(text):
    text = re.sub(r'(?::|;|=)(?:-)?(?:\)|\(|D|P)', '', text)
    return text


def strip_all_entities(text):
    text = re.sub(r'http\S+', "", text)
    text = re.sub(r'(@\w+)|#|&|!', "", text)
    text = text.replace('/', ' ')
    text = text.replace('\r', '').replace('\n', ' ').lower()
    text = re.sub(r'[^\x00-\x7f]',r'', text)
    text = re.sub(r'[0-9\.]+', '', text)
    banned_list= string.punctuation
    table = str.maketrans('', '', banned_list)
    text = text.translate(table)
    text = [word for word in text.split() if word not in stop_words]
    text = ' '.join(text)
    text =' '.join(word for word in text.split() if len(word) < 14)
    return text



def clean_hashtags(text):
    text = " ".join(word.strip() for word in re.split('#(?!(?:hashtag)\b)[\w-]+(?=(?:\s+#[\w-]+)*\s*$)', text))
    text = " ".join(word.strip() for word in re.split('#|_', text))
    return text

def decontract(text):
    text = re.sub(r"can\'t", "can not", text)
    text = re.sub(r"won\'t", "will not", text)

    text = re.sub(r"n\'t", " not", text)
    text = re.sub(r"\'re", " are", text)
    text = re.sub(r"\'s", " is", text)
    text = re.sub(r"\'d", " would", text)
    text = re.sub(r"\'ll", " will", text)
    text = re.sub(r"\'t", " not", text)
    text = re.sub(r"\'ve", " have", text)
    text = re.sub(r"\'m", " am", text)

    text = re.sub(r"cant", "can not", text)
    text = re.sub(r"wont", "will not", text)
    text = re.sub(r"wasnt", "was not", text)
    text = re.sub(r"ive", "i have", text)
    text = re.sub(r"dont", "do not", text)
    text = re.sub(r"didnt", "did not", text)
    text = re.sub(r"aim", "i am", text)
    text = re.sub(r"aint", "am not", text)
    text = re.sub(r"youre", "you are", text)
    text = re.sub(r"isnt", "is not", text)
    text = re.sub(r"isn", "is not", text)

    text = re.sub(r"thanx", "thank you", text)
    text = re.sub(r"shyt", "shit", text)
    text = re.sub(r"pls", "please", text)
    return text


def filter_chars(a):
    sent = []
    for word in a.split(' '):
        if ('$' in word) | ('&' in word):
            sent.append('')
        else:
            sent.append(word)
    return ' '.join(sent)


def remove_mult_spaces(text):
    return re.sub("\s\s+" , " ", text)


def stemmer(text):
    tokenized = nltk.word_tokenize(text)
    ps = SnowballStemmer("english")
    return ' '.join([ps.stem(words) for words in tokenized])

wl = WordNetLemmatizer()

def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN


def lemmatizer(string):
    word_pos_tags = nltk.pos_tag(word_tokenize(string))
    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)]

def preprocess(text):
    text = emoticon_remove(text)
    text = strip_all_entities(text)
    text = decontract(text)

    text = filter_chars(text)
    text = remove_mult_spaces(text)

    text = lemmatizer(text)
    text = re.sub(r"isi", "isis", text)
    return text

def my_stemmer(text):
    tokenized = nltk.word_tokenize(text)
    ps = SnowballStemmer("english")
    return ' '.join([ps.stem(words) for words in tokenized])

import nltk
nltk.download('averaged_perceptron_tagger')

import nltk
nltk.download('wordnet')

df['text_clean'] = df['Tweet'].apply(lambda x: preprocess(x))

df.head()

def tokenization(text):
    text = re.split('\W+', text)
    return text

df['Tweet_tokenized'] = df['text_clean'].apply(lambda x: tokenization(x))

df['tokenized_bigram'] = df['Tweet_tokenized'].apply(lambda words: list(nltk.ngrams(words, 2)))

df.head(10)

df.drop_duplicates("text_clean", inplace=True)

df.shape

"""### Frequencies"""

text = list()
for tweet in df['Tweet_tokenized']:
    for el in tweet:
        text.append(el)

freq_dist = nltk.FreqDist(text)

freq_dist.most_common(20)

freq_dist.plot(20)

nltk_text = nltk.Text(text)

"""# Word cloud & Bi-grams

### Not cyberbullying
"""

plt.figure(figsize=(20,10))
subset_notc = df[df['Type']==0]

text_notc = list()
for el in subset_notc.Tweet_tokenized.values:
    for i in el:
        text_notc.append(i)

cloud1 = WordCloud(background_color='black',colormap="Dark2",
                   collocations=False,width=1500,height=750).generate(" ".join(text_notc))

plt.axis('off')
#plt.title("Not-Bullying",fontsize=40)
plt.imshow(cloud1)

freq_notc = nltk.FreqDist(text_notc)
freq_notc.most_common(10)

bigram_notc = list()
for tweet in subset_notc['Tweet_tokenized']:
    for i in range (0, len(tweet)-1):
        bigram_notc.append((tweet[i], tweet[i+1]))

big_freq_notc = nltk.FreqDist(bigram_notc)

big_freq_notc.most_common(20)

big_freq_notc.plot(10)

"""### Religion"""

plt.figure(figsize=(20,10))
subset_rel = df[df['Type']==1]

text_rel = list()
for el in subset_rel.Tweet_tokenized.values:
    for i in el:
        text_rel.append(i)

cloud1 = WordCloud(background_color='black',colormap="Dark2",
                   collocations=False,width=1500,height=750).generate(" ".join(text_rel))

plt.axis('off')
#plt.title("Religion",fontsize=40)
plt.imshow(cloud1)

freq_rel = nltk.FreqDist(text_rel)
freq_rel.most_common(10)

#WORDS FREQUENCY _ BiGrams

bigram_rel = list()
for tweet in subset_rel['Tweet_tokenized']:
    for i in range (0, len(tweet)-1):
        bigram_rel.append((tweet[i], tweet[i+1]))

big_freq_rel = nltk.FreqDist(bigram_rel)

big_freq_rel.most_common(20)

big_freq_rel.plot(10)

"""### Age"""

plt.figure(figsize=(20,10))
subset_age = df[df['Type']==2]

text_age = list()
for el in subset_age.Tweet_tokenized.values:
    for i in el:
        text_age.append(i)

cloud1 = WordCloud(background_color='black',colormap="Dark2",
                   collocations=False,width=1500,height=750).generate(" ".join(text_age))

plt.axis('off')
#plt.title("Age",fontsize=40)
plt.imshow(cloud1)

freq_age = nltk.FreqDist(text_age)
freq_age.most_common(10)

#WORDS FREQUENCY _ BiGram

bigram_age = list()
for tweet in subset_age['Tweet_tokenized']:
    for i in range (0, len(tweet)-1):
        bigram_age.append((tweet[i], tweet[i+1]))

big_freq_age = nltk.FreqDist(bigram_age)

big_freq_age.most_common(20)

big_freq_age.plot(10)

"""### Gender"""

plt.figure(figsize=(20,10))
subset_gen = df[df['Type']==3]

text_gender = list()
for el in subset_gen.Tweet_tokenized.values:
    for i in el:
        text_gender.append(i)

cloud1 = WordCloud(background_color='black',colormap="Dark2",
                   collocations=False,width=1500,height=750).generate(" ".join(text_gender))

plt.axis('off')
plt.imshow(cloud1)

freq_gen = nltk.FreqDist(text_gender)
freq_gen.most_common(10)

#WORDS FREQUENCY _ 1Gram e 2Gram

bigram_gen = list()
for tweet in subset_gen['Tweet_tokenized']:
    for i in range (0, len(tweet)-1):
        bigram_gen.append((tweet[i], tweet[i+1]))

big_freq_gen = nltk.FreqDist(bigram_gen)

big_freq_gen.most_common(20)

big_freq_gen.plot(10)

"""### Ethnicity"""

plt.figure(figsize=(20,10))
subset_eth = df[df['Type']==4]

text_eth = list()
for el in subset_eth.Tweet_tokenized.values:
    for i in el:
        text_eth.append(i)

cloud1 = WordCloud(background_color='black',colormap="Dark2",
                   collocations=False,width=1500,height=750).generate(" ".join(text_eth))

plt.axis('off')
#plt.title("Ethnicity",fontsize=40)
plt.imshow(cloud1)

freq_eth = nltk.FreqDist(text_eth)
freq_eth.most_common(10)

#WORDS FREQUENCY _ 1Gram e 2Gram

bigram_eth = list()
for tweet in subset_eth['Tweet_tokenized']:
    for i in range (0, len(tweet)-1):
        bigram_eth.append((tweet[i], tweet[i+1]))

big_freq_eth = nltk.FreqDist(bigram_eth)

big_freq_eth.most_common(20)

big_freq_eth.plot(10)

df_clean = df.copy()

#cleaden_tweet is with stop_words

df_clean.to_csv("Data/cleaned_tweet.csv")

print(df.columns)


# -*- coding: utf-8 -*-
"""2.1 Binary Classification - Cyberbullying Tweets (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-gHqBnW3Z67iVKf8MEGfYtsEo3sKFHkj

# Binary Classification
"""

pip install --upgrade imbalanced-learn scikit-learn

pip install --user scikit-learn==1.4.1.post1 imbalanced-learn==0.12.0

pip install lime

import csv
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from stop_words import get_stop_words

from imblearn.over_sampling import RandomOverSampler, SMOTE
from imblearn.under_sampling import RandomUnderSampler

from imblearn.over_sampling import SMOTE

from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

#from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix


from sklearn.model_selection import learning_curve

#LIME
from sklearn.pipeline import make_pipeline
from lime.lime_text import LimeTextExplainer

from sklearn.metrics import confusion_matrix, classification_report

#from sklearn.metrics import plot_confusion_matrix

import sklearn
print(sklearn.__version__)

# stop_words = get_stop_words('english')
# stop_words.extend(['', 're', 'rt', 'didn', 'bc', 'n', 'm', 'im', 'll', 'y', 've', 'u', 'ur', 'don', 't', 's', 'b',
#                   'aren', 'can', 'couldn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'let', 'mustn', 'shan', 'shouldn',
#                    'wasn', 'weren', 'won', 'wouldn'])

# utility function
def make_confusion_matrix( cfm, title):
    group_names = ['TN','FP','FN','TP']

    group_counts = ["{0:0.0f}".format(value) for value in
                cfm.flatten()]

    group_percentages = ["{0:.2%}".format(value) for value in
                     cfm.flatten()/np.sum(cfm)]

    labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

    labels = np.asarray(labels).reshape(2,2)
    plt.title(title)

    sns.heatmap(cfm, annot=labels, fmt="", cmap='Blues')
    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted',fontsize=12)

from sklearn.metrics import roc_curve,auc

def plot_roc_curve(y_test, prediction, name_model):
    test_fpr, test_tpr, te_thresholds = roc_curve(y_test, prediction)

    plt.grid()
    auc_score = round(auc(test_fpr, test_tpr),2)
    plt.plot(test_fpr, test_tpr, label=f"{name_model} - AUC ="+ str(auc_score))
    plt.plot([0,1],[0,1],'r--')
    plt.legend()
    plt.xlabel("True Positive Rate")
    plt.ylabel("False Positive Rate")
    plt.title(f" AUC(ROC curve) - {name_model}")
    plt.grid(color='black', linestyle='', linewidth=0.5)
    plt.show()

"""## Import dataset and data preparation

"""

# df_test = pd.read_csv('test_set.csv')
# df_test = df_test.drop(columns=['Unnamed: 0'])
# df_test["Type"].replace("not_cyberbullying", "not_c", inplace=True)

# df_test.head()

df = pd.read_csv('Data/cleaned_tweet.csv')

df = df.drop(columns=[df.columns[0]])

print(df.columns)

unique = df["Type"].unique()
freq = df["Type"].value_counts()
sns.set(font_scale=1)

ax = sns.countplot(x="Type", data=df, order=freq.index)
plt.title("Target variable counts in dataset")
plt.ylabel('Number of tweets')
plt.xlabel('Tweet Type')

# Adding the text labels
rects = ax.patches
for rect, label in zip(rects, freq):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')

plt.show()

df = df.dropna()

df.loc[df.Type != 0,'Type'] = 1

(unique, counts) = np.unique(df['Type'], return_counts=True)

print('Unique values of the target variable', unique)
print('Counts of the target variable :', counts)

sns.set(font_scale=1)

ax = sns.barplot(x=unique, y=counts)
plt.title("Target variable counts in dataset")
plt.ylabel('Number of tweets')
plt.xlabel('Tweet Type')


rects = ax.patches
for rect, label in zip(rects, counts):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')

plt.show()

text_len = []
for text in df.Tweet:
    tweet_len = len(text.split())
    text_len.append(tweet_len)

df['text_len'] = text_len

df

plt.figure(figsize=(20,5))
ax = sns.countplot(x='text_len', data=df[df['text_len']<=1000], hue=df['Type'], palette='mako')
plt.title('Count of tweets with words distribution', fontsize=20)
plt.legend(['Not_cyberbullying', 'Cyberbullying'])
plt.yticks([200, 500, 1000, 1500], ['200','500', '1000', '1500'],rotation=45)
plt.ylabel('count')
plt.xlabel('number of words')
plt.show()

plt.figure(figsize=(16,5))
ax = sns.countplot(x='text_len', data=df[df['text_len']<=30],hue=df['Type'], palette='mako')
plt.title('Count of tweets with less than 30 words', fontsize=20)
plt.legend(['Not_cyberbullying', 'Cyberbullying'])
plt.yticks([500, 1000, 1500], ['500', '1000', '1500'],rotation=45)
plt.ylabel('count')
plt.xlabel('number of words')
plt.show()

#I remove all the tweet with less than 7 words
df = df[df['text_len'] > 7]

plt.figure(figsize=(16,5))
ax = sns.countplot(x='text_len', data=df[(df['text_len']<=1000) & (df['text_len']>30)], hue=df['Type'], palette='mako')
plt.title('Count of tweets with high number of words', fontsize=20)
plt.legend(['Not_cyberbullying', 'Cyberbullying'])
plt.yticks([])
ax.bar_label(ax.containers[1])
plt.ylabel('count')
plt.xlabel('number of words')
plt.show()

#I remove tweet with more than 45 words
df = df[df['text_len'] < 45]

max_len = np.max(df['text_len'])
max_len

df.sort_values(by=["text_len"], ascending=False)

(unique, counts) = np.unique(df['Type'], return_counts=True)

sns.set(font_scale=1)

ax = sns.barplot(x=unique, y=counts)
plt.title("Target variable counts in dataset")
plt.ylabel('Number of tweets')
plt.xlabel('Tweet Type')


rects = ax.patches
for rect, label in zip(rects, counts):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')

plt.show()

"""## Dataset split"""

data = 'Tweet_tokenized'
target = 'Type'
X_train, X_test, y_train, y_test = train_test_split(df[data], df[target], test_size=0.3, random_state=42)

print("x_train ->", len(X_train), "record")
print("x_test  ->", len(X_test), "record")
print("y_train ->", len(y_train), "record")
print("y_test  ->", len(y_test), "record")

y_train.value_counts()

"""## Class balancing"""

ros = RandomOverSampler(sampling_strategy=0.4)
X_train, y_train = ros.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train).reshape(-1, 1))
train_os = pd.DataFrame(list(zip([x[0] for x in X_train], y_train)), columns = ['Tweet_tokenized', 'Type'])
X_train = train_os['Tweet_tokenized'].values
y_train = train_os['Type'].values

rus = RandomUnderSampler(sampling_strategy='majority')
X_train, y_train = rus.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train).reshape(-1, 1))
train_os = pd.DataFrame(list(zip([x[0] for x in X_train], y_train)), columns = ['Tweet_tokenized', 'Type'])
X_train = train_os['Tweet_tokenized'].values
y_train = train_os['Type'].values

(unique, counts) = np.unique(y_train, return_counts=True)
np.asarray((unique, counts)).T

sns.set(font_scale=1)

ax = sns.barplot(x=unique, y=counts)
plt.ylabel('Number of tweets')
plt.xlabel('Tweet Type')

rects = ax.patches
for rect, label in zip(rects, counts):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')

plt.show()

"""## Tokenization"""

X_train

vect = CountVectorizer()#min_df = 5)
X_train_tok = vect.fit_transform(X_train)
X_test_tok =vect.transform(X_test)

len(vect.vocabulary_)

vect.vocabulary_

vect = CountVectorizer()
X_train_tok = vect.fit_transform(X_train)
X_test_tok = vect.transform(X_test)

feature_names = vect.get_feature_names_out()
print(feature_names)

X_train_tok[0,:]

print(X_train_tok[0,:])

vect.inverse_transform(X_train_tok[0,:])

for feat,freq in zip(vect.inverse_transform(X_train_tok[0,:])[0],X_train_tok[0,:].data):
    print(feat,freq)

"""## Feature selection

"""

bin_sel = SelectKBest(chi2, k = 2000).fit(X_train_tok,y_train)
X_train_sel_bin = bin_sel.transform(X_train_tok)
X_test_sel_bin = bin_sel.transform(X_test_tok)

bin_sel.get_support()

X_train_sel_bin

X_train_sel_bin[0,:]

print(X_train_sel_bin[0,:])

print(vect.inverse_transform(bin_sel.inverse_transform(X_train_sel_bin[0,:])))

"""## Weigthing



"""

tf_transformer_bin = TfidfTransformer(use_idf=True).fit(X_train_sel_bin)
X_train_tf_bin = tf_transformer_bin.transform(X_train_sel_bin)
X_test_tf_bin = tf_transformer_bin.transform(X_test_sel_bin)

print(X_train_tf_bin[0,:])

for feat,weight,freq in zip(vect.inverse_transform(bin_sel.inverse_transform(X_train_tf_bin[0,:]))[0],X_train_tf_bin[0,:].data,X_train_sel_bin[0,:].data):
    print(feat,weight,freq)

"""# Classification algorithms

## Naive Bayes
"""

bin_nb_clf = MultinomialNB()
bin_nb_clf.fit(X_train_tf_bin, y_train)
bin_nb_pred = bin_nb_clf.predict(X_test_tf_bin)

print('Classification report:')
print(classification_report(y_test, bin_nb_pred))

bin_nb_scores = cross_val_score(bin_nb_clf, X_train_tf_bin, y_train, cv=10)

print("%0.2f accuracy with a standard deviation of %0.2f" % (bin_nb_scores.mean(), bin_nb_scores.std()))

nb_cm = confusion_matrix(y_test, bin_nb_pred)
make_confusion_matrix(nb_cm, 'Naive Bayes - Confusion Matrix')

plot_roc_curve(y_test, bin_nb_pred, 'Naive Bayes')

"""## Linear SVC"""

bin_svm_clf = LinearSVC().fit(X_train_tf_bin, y_train)
bin_svm_pred = bin_svm_clf.predict(X_test_tf_bin)

print('Classification report:')
print(classification_report(y_test, bin_svm_pred))

bin_svm_cm = confusion_matrix(y_test, bin_svm_pred)
make_confusion_matrix(bin_svm_cm, 'Linear SVM - Confusion Matrix')

plot_roc_curve(y_test, bin_svm_pred, 'Linear SVM')

"""## Logistic Regression"""

bin_lr_clf = LogisticRegression(C = 0.5, solver = "sag").fit(X_train_tf_bin, y_train) #solver as 'sag' has better performance with large dataset
bin_lr_pred = bin_lr_clf.predict(X_test_tf_bin)

print('Classification report:')
print(classification_report(y_test, bin_lr_pred))

bin_lr_scores = cross_val_score(bin_lr_clf, X_train_tf_bin, y_train, cv=10)
print("\n \n%0.2f accuracy with a standard deviation of %0.2f" % (bin_lr_scores.mean(), bin_lr_scores.std()))

bin_lr_cm = confusion_matrix(y_test, bin_lr_pred)
make_confusion_matrix(bin_lr_cm, 'Logistic Regression - Confusion Matrix')

plot_roc_curve(y_test, bin_lr_pred, 'Logistic Regression')

"""## Decision Tree"""

bin_dt_clf = DecisionTreeClassifier(criterion = 'entropy', min_samples_leaf=20, min_samples_split=30).fit(X_train_tf_bin, y_train)
bin_dt_pred = bin_dt_clf.predict(X_test_tf_bin)

print('Classification report:')
print(classification_report(y_test, bin_dt_pred))

bin_dt_scores = cross_val_score(bin_dt_clf, X_train_tf_bin, y_train, cv=10)
print("\n \n%0.2f accuracy with a standard deviation of %0.2f" % (bin_dt_scores.mean(), bin_dt_scores.std()))

bin_dt_cm = confusion_matrix(y_test, bin_dt_pred)
make_confusion_matrix(bin_dt_cm, 'Decision tree - Confusion Matrix')

plot_roc_curve(y_test, bin_dt_pred, 'Decision Tree')

"""## Random Forest Classifier"""

bin_rf_clf = RandomForestClassifier(min_samples_leaf=20, min_samples_split=30).fit(X_train_tf_bin, y_train)
bin_rf_pred = bin_rf_clf.predict(X_test_tf_bin)

print('Classification report:')
print(classification_report(y_test, bin_rf_pred))

bin_rf_scores = cross_val_score(bin_rf_clf, X_train_tf_bin, y_train, cv=10)
print("\n \n%0.2f accuracy with a standard deviation of %0.2f" % (bin_rf_scores.mean(), bin_rf_scores.std()))

bin_rf_cm = confusion_matrix(y_test, bin_rf_pred)
make_confusion_matrix(bin_rf_cm, 'Random Forest - Confusion Matrix')

plot_roc_curve(y_test, bin_rf_pred, 'Random Forest')

df

"""## Calibration probabilities"""

from sklearn.calibration import CalibrationDisplay

class NaivelyCalibratedLinearSVC(LinearSVC):
    """LinearSVC with `predict_proba` method that naively scales
    `decision_function` output."""

    def fit(self, X, y):
        super().fit(X, y)
        df = self.decision_function(X)
        self.df_min_ = df.min()
        self.df_max_ = df.max()

    def predict_proba(self, X):
        """Min-max scale output of `decision_function` to [0,1]."""
        df = self.decision_function(X)
        calibrated_df = (df - self.df_min_) / (self.df_max_ - self.df_min_)
        proba_pos_class = np.clip(calibrated_df, 0, 1)
        proba_neg_class = 1 - proba_pos_class
        proba = np.c_[proba_neg_class, proba_pos_class]
        return proba

# Create classifiers
lr = LogisticRegression(penalty='l2', solver = 'sag', C=0.5)
gnb = MultinomialNB()
svc = NaivelyCalibratedLinearSVC(C=0.5)
rfc = RandomForestClassifier(min_samples_leaf=20, min_samples_split=30)

clf_list = [
    (lr, "Logistic"),
    (gnb, "Naive Bayes"),
    (svc, "SVC"),
    (rfc, "Random forest"),
]

from matplotlib.gridspec import GridSpec

fig = plt.figure(figsize=(10, 10))
gs = GridSpec(4, 2)
colors = plt.cm.get_cmap("Dark2")

ax_calibration_curve = fig.add_subplot(gs[:2, :2])
calibration_displays = {}
for i, (clf, name) in enumerate(clf_list):
    clf.fit(X_train_tf_bin, y_train)
    display = CalibrationDisplay.from_estimator(
        clf,
        X_test_tf_bin,
        y_test,
        n_bins=10,
        name=name,
        ax=ax_calibration_curve,
        color=colors(i),
    )
    calibration_displays[name] = display

ax_calibration_curve.grid()
ax_calibration_curve.set_title("Calibration plots")

# Add histogram
grid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]
for i, (_, name) in enumerate(clf_list):
    row, col = grid_positions[i]
    ax = fig.add_subplot(gs[row, col])

    ax.hist(
        calibration_displays[name].y_prob,
        range=(0, 1),
        bins=10,
        label=name,
        color=colors(i),
    )
    ax.set(title=name, xlabel="Mean predicted probability", ylabel="Count")

plt.tight_layout()
plt.show()

"""# Naive Bayes Explainability with LIME"""

## split to train and test
X_trainexp, X_val, y_trainexp, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)

## vectorize to tf-idf vectors
tfidf_vc = TfidfVectorizer(min_df = 10, max_features = 2000, analyzer = "word", ngram_range = (1, 2), lowercase = True)
train_vc = tfidf_vc.fit_transform(X_trainexp)
val_vc = tfidf_vc.transform(X_val)

model =MultinomialNB()
model = model.fit(train_vc, y_trainexp)
val_pred = model.predict(val_vc)

from sklearn.metrics import f1_score
val_cv = f1_score(y_val, val_pred, average = "binary")
print(val_cv)

idx = 142
c = make_pipeline(tfidf_vc, model)
class_names = ["Not_cyberbullying", "Cyberbullying"]
explainer = LimeTextExplainer(class_names = class_names)
exp = explainer.explain_instance(X_val[idx], c.predict_proba , num_features = 10)

print("Question: \n", X_val[idx])
print("Probability (Cyberbullying) =", c.predict_proba([X_val[idx]])[0, 1])
print("Probability (Not_cyberbullying) =", c.predict_proba([X_val[idx]])[0, 0])
print("True Class is:", class_names[y_val[idx]])

exp.as_list()

X_val[idx]

exp.show_in_notebook(text = X_val[idx], labels=(1,))

from collections import OrderedDict

weights = OrderedDict(exp.as_list())
lime_weights = pd.DataFrame({"words": list(weights.keys()), "weights": list(weights.values())})

sns.barplot(x = "words", y = "weights", data = lime_weights)
plt.xticks(rotation = 45)
plt.title("Sample {} features weights given by LIME".format(idx))
plt.show()


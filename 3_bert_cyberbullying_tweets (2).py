# -*- coding: utf-8 -*-
"""3_BERT_Cyberbullying_Tweets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B6CkrXMjtL6ORptfdzkPsK528FhFQ2yd

# Multiclass Classification task with BERT
"""

!pip install datasets

!pip install Transformers

import csv
import numpy as np
import pandas as pd
import re

from datasets import Dataset

from sklearn.model_selection import train_test_split


#Transformers BERT
import transformers
from transformers import BertModel
from transformers import BertTokenizer
from transformers import AdamW, get_linear_schedule_with_warmup

import tensorflow as tf
import tensorflow_hub as hub

import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

import time

from google.colab import output
output.enable_custom_widget_manager()

def text_preprocessing(text):
    """
    - Remove entity mentions (eg. '@united')
    - Correct errors (eg. '&amp;' to '&')
    @param    text (str): a string to be processed.
    @return   text (Str): the processed string.
    """
    # Remove '@name'
    text = re.sub(r'(@.*?)[\s]', ' ', text)

    # Replace '&amp;' with '&'
    text = re.sub(r'&amp;', '&', text)

    # Remove trailing whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text

df1 = pd.read_csv('cleaned_tweet.csv')
df1 = df1.drop(columns=['Unnamed: 0'])

df1= df1.dropna()

df1 = df1[['Tweet',"Type"]]

"""{"not_cyberbullying": 0, "religion": 1, "age": 2, "gender": 3, "ethnicity": 4}"""

possible_labels = sorted(list(df1.Type.unique()))
possible_labels

df1

df1['Tweet'] = df1['Tweet'].apply(lambda x: text_preprocessing(x))

df1

#df = df1.sample(frac=0.05, replace=True, random_state=42)
df = df1.copy()
df.shape

max_len = max([len(sent) for sent in df.Tweet])
print('Max length: ', max_len)

#TRAIN AND TEST
X_train, X_test, y_train, y_test = train_test_split(df.index.values,
                                                   df.Type.values,
                                                   test_size = 0.33,
                                                   random_state = 42,
                                                   stratify = df.Type.values)

df['data_type'] = ['not_set'] * df.shape[0]

df.loc[X_train, 'data_type'] = 'train'
df.loc[X_test, 'data_type'] = 'test'

#groupby count
df.groupby(['Type', 'data_type']).count()

"""## BERT Tokenization"""

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True) #o AutoTokenizer with bert-base-uncased

MAX_LENGTH= 128

#encode train set
encoded_data_train = tokenizer.batch_encode_plus(df[df.data_type == 'train'].Tweet.values,
                                                add_special_tokens = True,
                                                return_attention_mask = True,
                                                pad_to_max_length = True,
                                                max_length = MAX_LENGTH,
                                                return_tensors = 'pt')

#encode validation set
encoded_data_test = tokenizer.batch_encode_plus(df[df.data_type == 'test'].Tweet.values,
                                                add_special_tokens = True,
                                                return_attention_mask = True,
                                                pad_to_max_length = True,
                                                max_length = MAX_LENGTH,
                                                return_tensors = 'pt')

#train set
input_ids_train = encoded_data_train['input_ids']
attention_masks_train = encoded_data_train['attention_mask']
labels_train = torch.tensor(df[df.data_type == 'train'].Type.values)

#test set
input_ids_test = encoded_data_test['input_ids']
attention_masks_test = encoded_data_test['attention_mask']
labels_test = torch.tensor(df[df.data_type == 'test'].Type.values)

"""## BERT pre-trained model"""

from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained('bert-base-uncased',
                                                      num_labels = len(possible_labels),
                                                      output_attentions = False,
                                                      output_hidden_states = False)

#Create dataloaders
from torch.utils.data import TensorDataset

#train set
dataset_train = TensorDataset(input_ids_train,
                              attention_masks_train,
                              labels_train)

#test set
dataset_test = TensorDataset(input_ids_test,
                             attention_masks_test,
                             labels_test)

from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

batch_size = 16

#train set
dataloader_train = DataLoader(dataset_train,
                              sampler = RandomSampler(dataset_train),
                              batch_size = batch_size)

#validation set
dataloader_test = DataLoader(dataset_test,
                              sampler = RandomSampler(dataset_test),
                              batch_size = batch_size)

#Set Up Optimizer and Scheduler

from transformers import AdamW, get_linear_schedule_with_warmup

optimizer = AdamW(model.parameters(),
                 lr = 1e-5,
                 eps = 1e-8)

epochs = 2

scheduler = get_linear_schedule_with_warmup(optimizer,
                                           num_warmup_steps = 0,
                                           num_training_steps = len(dataloader_train)*epochs)

#Define evaluation performance

def evaluate(dataloader_val):


    model.eval()


    loss_val_total = 0
    predictions, true_vals = [], []

    for batch in tqdm_notebook(dataloader_val):


        batch = tuple(b.to(device) for b in batch)

        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2]}


        with torch.no_grad():
            outputs = model(**inputs)

        loss = outputs[0]
        logits = outputs[1]
        loss_val_total += loss.item()


        logits = logits.detach().cpu().numpy()
        label_ids = inputs['labels'].cpu().numpy()
        predictions.append(logits)
        true_vals.append(label_ids)

    loss_val_avg = loss_val_total/len(dataloader_val)

    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)

    return loss_val_avg, predictions, true_vals

#F1 Score
import numpy as np
from sklearn.metrics import f1_score

def f1_score_func(preds, labels):
    preds_flat = np.argmax(preds, axis = 1).flatten()
    labels_flat = labels.flatten()
    return f1_score(labels_flat, preds, average = 'weighted')

#accuracy score
def accuracy_per_class(preds, labels):

    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()

    for label in np.unique(labels_flat):
        y_preds = preds_flat[labels_flat==label]
        y_true = labels_flat[labels_flat==label]
        print(f'Class: {possible_labels[label]}')
        print(f'Accuracy:{len(y_preds[y_preds==label])}/{len(y_true)}\n')

"""## Train model"""

import random

seed_val = 17
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

from tqdm.notebook import tqdm_notebook
import torch


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

for epoch in tqdm_notebook(range(1, epochs+1)):
    model.to(device)
    model.train()

    loss_train_total = 0

    progress_bar = tqdm_notebook(dataloader_train,
                        desc = 'Epoch {:1d}'.format(epoch),
                        leave = False,
                        disable = False)

    for batch in progress_bar:

        model.zero_grad() #set gradient to 0

        batch = tuple(b.to(device) for b in batch)

#input will take three input: ids, attention_mask and labels
        inputs = {'input_ids': batch[0],
                  'attention_mask': batch[1],
                  'labels': batch[2]}

        outputs = model(**inputs) #unpack the dict straight into inputs

        loss = outputs[0]
        loss_train_total += loss.item()
        loss.backward()
#clips the norm of the gradients to 1.0 to prevent exploding gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step() #updates the modelâ€™s parameters
        scheduler.step() #updates learning rate
        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch))})

    tqdm_notebook.write(f'\n Epoch {epoch}')

    loss_train_ave = loss_train_total / len(dataloader_train)
    tqdm_notebook.write(f'Training loss: {loss_train_ave}')

    val_loss, predictions, true_vals = evaluate(dataloader_test)

    tqdm_notebook.write(f'Validation loss: {val_loss}')

_, predictions, true_vals = evaluate(dataloader_test)

accuracy_per_class(predictions, true_vals)

